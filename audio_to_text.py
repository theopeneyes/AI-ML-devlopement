# # -*- coding: utf-8 -*-
# """audio-to-text.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1qIUpp5tT-6wmCCd2_jFJ5IJLGm-0ncDv
# """

# from google.colab import drive
# drive.mount("/content/drive")

# !pip install transformers[torch] datasets[audio]
# !pip install librosa
# !pip install tiktoken

# !pip install accelerate
# !pip install tqdm

from transformers import pipeline
import librosa
import torch
import huggingface_hub as hub
import time
import re
import pandas as pd
import os
import re

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    # BitsAndBytesConfig,
)
base_path = "/home/ec2-user/AIG_Llama2/temp_files_audio"

audio_files = os.listdir(os.path.join(base_path, "AudioFiles"))
if not os.path.exists(os.path.join(base_path, "AudioLogs")):
    os.makedirs(os.path.join(base_path, "AudioLogs"))

df = pd.DataFrame(columns=[
    "AUDIO_FILE_NAME",
    "ASR_MODEL_NAME",
    "TG_MODEL_NAME",
    "ASR_LOADING_DURATION",
    "ASR_S2T",
    "TG_LOADING_DURATION",
    "TG_DURATION",
    "TOTAL_DURATION",
    "S2T_OUTPUT",
    "GeneratedQNA",
])

ultra_offset = time.time()

hub.login(token="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")
checkpoint = "mistralai/Mistral-7B-Instruct-v0.2"
audio_checkpoint = "openai/whisper-tiny"

# config = BitsAndBytesConfig(
#    load_in_4bit=True,
#    bnb_4bit_quant_type="nf4",
#    bnb_4bit_use_double_quant=True,
#    bnb_4bit_compute_dtype=torch.bfloat16
# )

# loading models required for compute
offset = time.time()

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
whisper = pipeline("automatic-speech-recognition", device = device,
                        model=audio_checkpoint)

audio_load_duration = time.time() - offset

offset = time.time()
mistral_tokenizer = AutoTokenizer.from_pretrained(checkpoint)
mistral = AutoModelForCausalLM.from_pretrained(
    checkpoint, # quantization_config=config,
    device_map=device,
)
model_load_duration = time.time() - offset

mistral_pipe = pipeline("text-generation",
                        model=mistral, tokenizer=mistral_tokenizer,do_sample=True,
                        max_new_tokens=1024, device_map=device, temperature=0.1)

ultra_duration = time.time() - ultra_offset

for file_name in audio_files:
    generation_offset = time.time()
    filepath = os.path.join(base_path, "AudioFiles", file_name)
    audio, sr = librosa.load(filepath, sr=16_000)
    offset = time.time()
    output = whisper(
        audio,
        return_timestamps=True,
        generate_kwargs = {"task": "transcribe"},
        chunk_length_s=30,
    )

    audio_to_text_duration = time.time() - offset

    item = output["text"]

    prompt = "Generate as many as possible difficulty level hard Multiple choice questions with four options and answer using this text:"+" "+item+'''\n Give me output in this JSON array format:[{"question": string, "options":List[string], "answer":string}]'''
    prompt_template= f'''[INST] <s> You have Phd in history and best in generating Multiple choice questions with demanded JSON format. </s> {prompt}[/INST]'''

    offset = time.time()
    mcqs = mistral_pipe(prompt_template)
    text_to_qna_duration = time.time() - offset

    generation_duration = time.time() - generation_offset
    loading_minutes, loading_seconds = divmod(ultra_duration, 60)
    generation_minutes, generation_seconds = divmod(generation_duration, 60)
    print("total duration audio to qna : %d mins %d seconds" % (generation_minutes, generation_seconds))

    pattern = r"{(?:[^{}]|)*}"
    matches = re.findall(pattern, mcqs[0]["generated_text"])
    qnas = "\n".join(matches) if matches != [] else "No qna generated"

    dirs = filepath.split("/")
    df1 = pd.DataFrame({
        "AUDIO_FILE_NAME": [dirs[-2] + "/" + dirs[-1]],
        "ASR_MODEL_NAME": [audio_checkpoint],
        "TG_MODEL_NAME": [checkpoint],
        "ASR_LOADING_DURATION": ["{} mins {:.2f} seconds".format(*divmod(audio_load_duration, 60))],
        "ASR_S2T": ["{} mins {:.2f} seconds".format(*divmod(audio_to_text_duration, 60))],
        "TG_LOADING_DURATION": ["{} mins {:.2f} seconds".format(*divmod(model_load_duration, 60))],
        "TG_DURATION": ["{} mins {:.2f} seconds".format(*divmod(text_to_qna_duration, 60))],
        "TOTAL_DURATION": ["{} mins {:.2f} seconds".format(*divmod(generation_duration, 60))],
        "S2T_OUTPUT": output["text"],
        "GeneratedQNA": [qnas],
    })

    df = pd.concat([df, df1])
    df.to_excel(os.path.join(base_path, "AudioLogs/AudioLogs.xlsx"), index=False)

df.to_excel(os.path.join(base_path, "AudioLogs.xlsx"), index=False)

df

df

